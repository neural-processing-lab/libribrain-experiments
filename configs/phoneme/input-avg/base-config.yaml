data:
  datasets:
    train:
      - libribrain_phoneme:
          data_path: '<DATA_PATH>'
          preprocessing_str: 'bads+headpos+sss+notch+bp+ds'
          label_type: 'phoneme'
          standardize: true
          tmin: 0.0
          tmax: 0.5
          exclude_run_keys: [['0', '11', 'Sherlock1', '2'], ['0', '12', 'Sherlock1', '2']]
          preload_files: false
          
    val:
      - libribrain_phoneme:
          data_path: '<DATA_PATH>'
          label_type: 'phoneme'
          standardize: true
          tmin: 0.0
          tmax: 0.5
          include_run_keys: [['0', '11', 'Sherlock1', '2']]
          preload_files: false
    test:
      - libribrain_phoneme:
          data_path: '<DATA_PATH>'
          label_type: 'phoneme'
          standardize: true
          tmin: 0.0
          tmax: 0.5
          include_run_keys: [['0', '12', 'Sherlock1', '2']]
          preload_files: false
  dataloader: 
    batch_size: 256
    num_workers: 4
  general:
    inMemory: False
    grouped_samples: 5


model:
  - average_groups:
        n_groups: "auto"
  - conv1d:
      in_channels: 306
      out_channels: 128
      kernel_size: 7
      stride: 1
      padding: 'same'
  - resnet_block:
      model_config:
        - elu:
        - conv1d:
            in_channels: 128
            out_channels: 128
            kernel_size: 3
            stride: 1
            padding: 'same'
        - elu:
        - conv1d:
            in_channels: 128
            out_channels: 128
            kernel_size: 1
            stride: 1
            padding: 'same'
  - elu:
  - conv1d:
      in_channels: 128
      out_channels: 128
      kernel_size: 50
      stride: 25
  - elu:
  - conv1d:
      in_channels: 128
      out_channels: 128
      kernel_size: 7
      stride: 1
      padding: 'same'
  - elu:
  - conv1d:
      in_channels: 128
      out_channels: 512
      kernel_size: 4
      padding: 0
  - relu:
  - dropout:
      p: 0.5
  - conv1d:
      in_channels: 512
      out_channels: 39
      kernel_size: 1
      padding: 0
  - flatten:

loss:
  name: cross_entropy
  
  
optimizer:
  name: adam
  config:
    lr: 0.00025
    
trainer:
  max_epochs: 30

general:
  wandb: True
  output_path: '<RESULTS_PATH>/final-input-avg'
  checkpoint_path: '<CHECKPOINTS_PATH>/final-input-avg'
  seed: 42
  best_model_metric: 'val_loss'